---
title: "California Housing Classification and Regression R Notebook"
output: html_notebook
---
There are 20640 rows in the dataset with 10 attributes. The description of each of these as given on kaggle is as below:

**longitude:** A measure of how far west a house is; a higher value is farther west
**latitude:** A measure of how far north a house is; a higher value is farther north
**housing_median_age:** Median age of a house within a block; a lower number is a newer building
**total_rooms:** Total number of rooms within a block
**total_bedrooms:** Total number of bedrooms within a block
**population:** Total number of people residing within a block
**households:** Total number of households, a group of people residing within a home unit, for a block
**median_income:** Median income for households within a block of houses (measured in tens of thousands of US Dollars)
**median_house_value:** Median house value for households within a block (measured in US Dollars)

```{r}
housing=read.csv('C:\\Users\\Rushi\\Downloads\\CSVs\\california_housing_train.csv')
housing
```
## List of libraries used
```{r}
library(psych)
library(corrplot)
library(Hmisc)
library(data.table)
library(mltools)
library(knitr)
library(caTools)
library(caret)
library(MASS)
```

## 2. Data Cleaning
Missing Data
As mentioned in the previous section, there are 207 observations with NA values for “total_bedrooms”. We can handle this by imputing median value (435) at these places.
```{r}
visdat::vis_dat(housing)
sum(is.na(housing))
```
## 3. Exploratory Data Analysis
After we have cleaned the data, we will visualize the data to get some insights into the distribution and skewness of numeric data as well as correlation of the variables with each other.

Histograms for numeric variables:
```{r}
par(mfrow = c(3, 3))
hist(housing$longitude, main = "longitude", col="slateblue1")
hist(housing$latitude, main = "latitude", col="slateblue1")
hist(housing$housing_median_age, main = "housing_median_age", col="slateblue1")
hist(housing$total_rooms, main = "total_rooms", col="slateblue1")
hist(housing$total_bedrooms, main = "total_bedrooms", col="slateblue1")
hist(housing$population, main = "population", col="slateblue1")
hist(housing$households, main = "households", col="slateblue1")
hist(housing$median_income, main = "median_income", col="slateblue1")
hist(housing$median_house_value, main = "median_house_value", col="slateblue1")
```
```{r}
library(ggplot2)
plot_map = ggplot(housing, 
                  aes(x = longitude, y = latitude, color = median_house_value, 
                      hma = housing_median_age, tr = total_rooms, tb = total_bedrooms,
                      hh = households, mi = median_income)) +
              geom_point(aes(size = population), alpha = 0.4) +
              xlab("Longitude") +
              ylab("Latitude") +
              ggtitle("Data Map - Longtitude vs Latitude and Associated Variables") +
              theme(plot.title = element_text(hjust = 0.5)) +
              scale_color_distiller(palette = "Paired") +
              labs(color = "Median House Value (in $USD)", size = "Population")
plot_map
```
## Training and Test Data
First we want to split the data into training and testing data. We will use an 70/30 split of a randomized sample. We will use a set seed to get repeatability.
```{r}
library(caret)
set.seed(420)
housing_trn_idx = createDataPartition(housing$median_house_value, p = .70, list = FALSE)
housing_trn_data = housing[housing_trn_idx, ]
housing_tst_data = housing[-housing_trn_idx, ]
```


Note that we decided to partition using ocean_proximity since this is a predictor that we believe will play a large role in predicting housing prices.
## Model Selection
### Using all Model Subsets
As a starting point we want to get an idea which parameters and their interactions are good ones to use in a potential model. To start this, we will create three initial models that will be fitted on our training data. The first model will be an additive model that utilizes all variables. The second model will be a two-way model that uses all variables as well as their two-way interactions. The third, and final, initial model will be a three-way model that uses all variables as well as their two and three-way interactions. We will create these models below.
```{r}
full_additive_model = lm(median_house_value ~ ., data = housing_trn_data)
full_additive_adjr2 = summary(full_additive_model)$adj.r.squared

full_twoway_model = lm(median_house_value ~ (.)^2, data = housing_trn_data)
full_twoway_adjr2 = summary(full_twoway_model)$adj.r.squared

full_threeway_model = lm(median_house_value ~ (.)^3, data = housing_trn_data)
full_threeway_adjr2 = summary(full_threeway_model)$adj.r.squared
```

In order to take a look at initial results of these models, we will be using model selection criterion AIC and Adj.R2. We will also include the total number of predictors in each model to get a feel of the total complexity of each.
```{r}
beginning_mods_results = data.frame(
  "Total Predictors" =
    c("Additive Model" = extractAIC(full_additive_model)[1],
      "Two-Way Int. Model" = extractAIC(full_twoway_model)[1],
      "Three-Way Int. Model" = extractAIC(full_threeway_model)[1]),
  "AIC" =
    c("Additive Model" = extractAIC(full_additive_model)[2],
      "Two-Way Int. Model" = extractAIC(full_twoway_model)[2],
      "Three-Way Int. Model" = extractAIC(full_threeway_model)[2]),
  "Adj R-Squared" =
    c("Additive Model" = full_additive_adjr2,
      "Two-Way Int. Model" = full_twoway_adjr2,
      "Three-Way Int. Model" = full_threeway_adjr2))

knitr::kable(beginning_mods_results, align = c("c", "r"))
```
We see that the model with the best (i.e., lowest) AIC is full_threeway_model, with a score of 3.18×105. However, although this model has the lowest AIC, it also has far more predictors (and therefore is more complex) than the other three models - 204 compared to 64 predictors for full_twoway_model, and 12 predictors for full_additive_model. This is something to keep in mind as we move forward, as model complexity should be taken into account.

```{r}
library(psych)
pairs.panels(housing[-1])
```
```{r}
library('e1071')
nb=naiveBayes(median_house_value ~ ., housing_trn_data)
summary(nb)
```
## Classification
```{r}
predict(nb, housing_tst_data, type = "class")
nb %class% housing_tst_data
```

```{r}
predict=predict(nb,housing_tst_data, type = "class")
actuals_preds <- data.frame(cbind(actuals=housing_tst_data$median_house_value, predicteds=predict)) 
head(actuals_preds)
```

