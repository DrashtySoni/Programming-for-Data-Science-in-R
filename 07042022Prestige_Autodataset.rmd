---
title: "R Notebook"
output: html_notebook
---

# **PRESTIGE Dataset**
---
```{r}
library(car)
df=Prestige
df
```
## Correlation Matrix on Prestige Dataset
```{r}
cor(Prestige[,-6]) # Correlation of Prestige Dataset on numeric variables
```

```{r}
library(corrplot)
corrplot(cor(Prestige[,-6]) , method = "number") # Plotting Correlation Matrix
```
## Let’s see the relationship of prestige against income, education & women variables through scatter plot
```{r}
library(ggplot2)
library(cowplot)
plot_income <- ggplot(data = Prestige, aes(x = prestige, y = income, col = type)) + geom_point()
plot_education <- ggplot(data = Prestige, aes(x = prestige, y = education, col = type)) + geom_point()
plot_women <- ggplot(data = Prestige, aes(x = prestige, y = women, col = type)) + geom_point()
plot_census <- ggplot(data = Prestige, aes(x = prestige, y = census, col = type)) + geom_point()
plot_grid(plot_income, plot_education, plot_women, plot_census, labels = "AUTO")
```
**We can see a strong linear relationship of prestige with income and education rather than women and census.** 

## Let’s take a look into the data distribution of income & education variables through historgram plot and compare against mean and median values
```{r}
hist_income <- ggplot(Prestige, aes(x = income)) + geom_histogram(binwidth = 1000) +
  geom_vline(xintercept = mean(Prestige$income), color = "indianred") +
  geom_vline(xintercept = median(Prestige$income), color = "cornflowerblue")
hist_education <- ggplot(Prestige, aes(x = education)) + geom_histogram(binwidth = 1) +
  geom_vline(xintercept = mean(Prestige$education), color = "indianred") +
  geom_vline(xintercept = median(Prestige$education), color = "cornflowerblue")
plot_grid(hist_income, hist_education, labels = "AUTO")
```
**We can see that income variable is right skewed distribution and education is also not representing normal distribution.** 

## Let’s try to transform this into normal distribution if possible. We will be using Log2 for income variable and scale the value of the variable education on its mean
```{r}
# Comparing original income histogram against log of income histogram
hist_income <- ggplot(Prestige, aes(x = income)) + geom_histogram(binwidth = 1000) +
  labs(title = "Original Income") + labs(x ="Income") +
  geom_vline(xintercept = mean(Prestige$income), color = "indianred") +
  geom_vline(xintercept = median(Prestige$income), color = "cornflowerblue")
hist_trnsf_income <- ggplot(Prestige, aes(x = log(income))) + geom_histogram(binwidth = 0.5) +
  labs(title = "Transformed Income") + labs(x ="Log of Income") +
  geom_vline(xintercept = mean(log(Prestige$income)), color = "indianred") +
  geom_vline(xintercept = median(log(Prestige$income)), color = "cornflowerblue")
plot_grid(hist_income, hist_trnsf_income, labels = "AUTO")
```
## List the class of the feature variables in the table
```{r}
typeof(Prestige[,-6])
```
## Give the descriptive statistics of the quantitative variables
```{r}
dim(df)

#Displays the type and a preview of all columns as a row so that it's very easy to take in.
library(dplyr)
glimpse(df)
```
Summary Statistics
It shows the summary statistics for numerical values
```{r}
summary(df)
```
Use Skimr to Display Summary Statistics for the variables
This has same as above + missing and a histogram.  Also, it has some additional statistics of non-numerical values.
```{r}
library(skimr)
skim(df)
```
## Plot the relationship between income and education
```{r}
ggplot(Prestige, aes(income,education, col=type)) + geom_point() + ggtitle('Relationship between Income and Education')
```
##  Perform a simple linear regression with income as the dependent variable and education as predictor variable.

Let’s build linear regression model step by step and eliminate the variables that are not significant to our model in the process to improve the performance of regression model. This will also correspond to our findings above of women and census variable not having realtionship with prestige
```{r}
# Fit a linear model with education, income, women & census variables
lm_mod1 = lm(prestige ~ education + log(income) + women + census, data = Prestige)
summary(lm_mod1) # run a summary of its results
```
**You can notice that Adjusted R2 is 82% which is good, however, p-value is high for women and census variables. p-Value: we can consider a linear model to be statistically significant only when p-Values are less than the pre-determined statistical significance level of 0.05. However, income and education has very significant p-value impacting the model. We have seen this correlation above also in scatterplot and correlation plot. Let’s remove the women and census variables, build regression model and check it’s summary.**

## Give your study on the model and the significance of the model
```{r}
# Fit a linear model with education and income variables
lm_mod2 = lm(prestige ~ education + log(income), data = Prestige)
summary(lm_mod2) # run a summary of its results
```
**Adjusted R2 is still 82%, but, the “intercept” is negative i.e. -95. To handle this negative intercept, we will add a new variable education.c that will center the value of the variable education on its mean. This transformation was applied on the education variable so we could have a meaningful interpretation of its intercept estimate.**
```{r}
prstg_df = Prestige # creating a new dataset copy of Prestige or other manipulations

# scaling the value of education to its mean value
set.seed(1)
education.c = scale(prstg_df$education, center=TRUE, scale=FALSE)
prstg_df = cbind(prstg_df, education.c)

# Fit a linear model with centered education and income variables
lm_mod3 = lm(prestige ~ education.c + log(income), data = prstg_df)
summary(lm_mod3) # run a summary of its results
```
```{r}
par(mfrow = c(2, 2))  # Split the plotting panel into a 2 x 2 grid
plot(lm_mod3)  # Plot the income model information
```
**Let’s see the residual values and plot of the last regression model built. Residuals are basically the difference between actual and predicted values. From above summary of model, residuals are ranging from -17 to 18 and you can see in the below plot that they are evenly distributed**

```{r}
# Compare Actual, Predicted and Residual values of prestige from Education model
prstg_df_pred = as.data.frame(prstg_df$prestige) # Save the actual values
prstg_df_pred$predicted <- predict(lm_mod3) # Save the predicted values
prstg_df_pred$residuals <- residuals(lm_mod3) # Save the residual values
head(prstg_df_pred)
```
```{r}
plot(residuals(lm_mod3)) # Residual distribution of the model
abline(a=0,b=0,col='blue')
```
Now, let’s see if we can improve the model. If you remember, above we had done scatterplot for income and education against prestige with “type” variable as category and you have seen that for each category the linearity is different. Let’s look into that wiht a different perspective. But, before that let’s handle the NA values in “type” variable.
```{r}
prstg_df <- na.omit(prstg_df) # remove rows containing na's values via omit function

ggplot_income <- ggplot(data = prstg_df, aes(x = prestige, y = income, col = type)) + geom_smooth()
ggplot_educ <- ggplot(data = prstg_df, aes(x = prestige, y = education, col = type)) + geom_smooth()
plot_grid(ggplot_income, ggplot_educ, labels = "AUTO")
```

# **AUTO DATASET**
---
```{r}
library(ISLR)
Auto
```
## Compute the matrix of correlations between the quantitative variables using the function cor().
```{r}
cor(Auto[,-9])
corrplot(cor(Auto[,-9]),method = c('circle'))
```
## Produce a scatterplot matrix which includes all of the variables in the data set using pairs()
```{r}
pairs(Auto)
```
## Identify the quantitative variables.
```{r}
str(Auto)
skim(Auto)
```
## Perform a multiple linear regression with mpg as the response and all other variables except name as the predictors.
```{r}
colnames(Auto)
# Fit a linear model with education, income, women & census variables
lm_mod2 = lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + year + origin, data = Auto)
summary(lm_mod2) # run a summary of its results
```
## Is there a relationship between the predictors and the response variable?
```{r}
plot_cylinder <- ggplot(data = Auto, aes(x = mpg, y = cylinders)) + geom_point() +col
plot_displacement <- ggplot(data = Auto, aes(x = mpg, y = displacement)) + geom_point()
plot_horsepower <- ggplot(data =Auto, aes(x = mpg, y = horsepower)) + geom_point()
plot_weight <- ggplot(data = Auto, aes(x = mpg, y = weight)) + geom_point()
plot_grid(plot_cylinder, plot_displacement, plot_horsepower, plot_weight, labels = "AUTO")

```
**We can infer that horsepowe, weight and displacement are perfect negatively correlated with mpg**

## Which predictors appear to have a statistically significant relationship to the response?
> displacement, weight, year and origin have a statistically significant relationship to the response

## What does the coefficient for the ‘year’ variable suggest?
> The variable is responsible for change in the dependent variable.

